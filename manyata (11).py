# -*- coding: utf-8 -*-
"""manyata.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RjFHBjqTeFcRnAwV7D2QpODgPAC1G0dL
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import numpy as np
import pandas as pd
from lightgbm import LGBMRegressor
from datetime import datetime, timedelta
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

manyata=pd.read_csv("/content/manyata_tech_park_data.csv")

manyata.info()

manyata['Date'] = pd.to_datetime(manyata['Date'],errors='coerce')

w=pd.read_csv("/content/weather_combined.csv")

w.info()

w['date']=pd.to_datetime(w['date'])

w.rename(columns={'date': 'Date'}, inplace=True)

merged_data = pd.merge(manyata, w, on='Date', how='left')

merged_data.info()

merged_data.head()

t=merged_data

t.isnull().sum()

def plot_distributions(df, columns):
    """
    Plots distribution plots for the specified columns in a DataFrame.

    Parameters:
    df (DataFrame): The DataFrame containing the data.
    columns (list): List of column names to plot distributions for.
    """
    plt.figure(figsize=(15, 10))  # Set the figure size for better visibility

    for i, col in enumerate(columns, 1):
        plt.subplot(2, 3, i)  # Create a subplot grid (2 rows, 3 columns)
        sns.histplot(df[col], kde=True)  # Plot the distribution with KDE curve
        plt.title(f'Distribution of {col}')
        plt.xlabel(col)
        plt.ylabel('Frequency')

    plt.tight_layout()  # Adjust subplots to fit in the figure area
    plt.show()

# List of columns to plot
columns_to_plot = ['tavg', 'tmin', 'tmax', 'prcp', 'Electricity_Reading']

# Call the function with the DataFrame and the list of columns
plot_distributions(t, columns_to_plot)

def fill_missing_values(df):
    """
    Fill missing values for specific columns in the DataFrame with different imputation strategies.

    Columns and Strategies:
    - 'tavg': Fill with the median.
    - 'tmax': Fill with the median.
    - 'tmin': Fill with the mean.

    Parameters:
    df (DataFrame): The DataFrame with missing values to be filled.

    Returns:
    DataFrame: The DataFrame with missing values filled.
    """
    df['tavg'].fillna(df['tavg'].median(), inplace=True)
    df['tmax'].fillna(df['tmax'].median(), inplace=True)
    df['tmin'].fillna(df['tmin'].mean(), inplace=True)

    return df

# Call the function to fill missing values
t = fill_missing_values(t)

t.isnull().sum()

final=t

final.to_csv('final.csv', index=False)

encoder = LabelEncoder()
column_mapping={}
for column in ['Tenant', 'Industry_Type']:
    t[column] = encoder.fit_transform(t[column])
    column_mapping[column] = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))

t.corr()

tsa=t

t.drop(columns=['Base_Electricity_Reading','prcp','tmin','tmax'], inplace=True)

def plot_feature_relationships(df, target_column='Electricity_Reading'):
    """
    Plots the correlation between each feature and the target column (Electricity_Reading) as a bar chart.

    Parameters:
    df (DataFrame): The DataFrame containing the features and target.
    target_column (str): The target column to calculate correlation against.
    """
    # Calculate the correlation of all columns with the target column
    correlations = df.corr()[target_column].drop(target_column)

    # Sort correlations by absolute value to show the strongest relationships at the top
    correlations_sorted = correlations.abs().sort_values(ascending=False)

    # Create a bar plot to show the relationships
    plt.figure(figsize=(6, 8))
    sns.barplot(x=correlations_sorted.index, y=correlations_sorted.values, palette='Blues_d')

    # Add plot title and labels
    plt.title(f'Correlation of Features with {target_column}', fontsize=16)
    plt.xticks(rotation=90, fontsize=12)
    plt.ylabel('Correlation Coefficient', fontsize=14)
    plt.xlabel('Features', fontsize=14)

    plt.tight_layout()
    plt.show()

# Call the function to plot feature relationships with 'Electricity_Reading'
plot_feature_relationships(t, target_column='Electricity_Reading')

def detect_and_visualize_outliers(df, columns):
    """
    Detects and visualizes outliers for specified columns using box plots.

    Parameters:
    df (DataFrame): The DataFrame containing the features to check for outliers.
    columns (list): A list of column names for which to detect and visualize outliers.

    Returns:
    outliers_info (dict): A dictionary containing the number of outliers for each column.
    """
    outliers_info = {}

    for column in columns:
        # Calculate Q1 (25th percentile) and Q3 (75th percentile)
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1

        # Define outlier boundaries
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Count outliers
        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
        outliers_info[column] = len(outliers)

        # Visualization using box plot
        plt.figure(figsize=(4, 4))
        sns.boxplot(x=df[column], palette='Set2')
        plt.title(f'Outliers in {column}', fontsize=16)
        plt.xlabel(column, fontsize=12)
        plt.show()

    return outliers_info

# Example usage:
columns_to_check = ['tavg','Number_of_Floors','Day_of_Week','Number_of_Employees','Acquired_Area' ,'Electricity_Reading',]
outliers_info = detect_and_visualize_outliers(t, columns_to_check)

# Output the number of outliers in each column
for col, count in outliers_info.items():
    print(f'Total number of outliers in {col}: {count}')

def print_outliers(df, columns):
    """
    Prints the outliers for each specified column based on the IQR method.

    Parameters:
    df (DataFrame): The DataFrame containing the features to check for outliers.
    columns (list of str): A list of column names for which to detect and print outliers.

    Returns:
    outliers_dict (dict): A dictionary containing DataFrames with the outlier values for each column.
    """
    outliers_dict = {}

    for column in columns:
        # Calculate Q1 (25th percentile) and Q3 (75th percentile)
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1

        # Define outlier boundaries
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Filter out the outliers
        outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]

        # Store outliers in the dictionary
        outliers_dict[column] = outliers[[column]]

        # Print outlier values
        print(f"Outliers in {column}:")
        print(outliers[[column]])
        print()  # Add a blank line for better readability between columns

    return outliers_dict

# Example usage:
columns = ['Electricity_Reading', 'tavg']
outliers_dict = print_outliers(t, columns)

t.drop(index=3389, inplace=True)

# Optionally, you can reset the index if needed
t.reset_index(drop=True, inplace=True)

temp=t.drop(columns=['Tenant','Date','Industry_Type'])

# Calculate skewness for relevant columns
skewness_info = {}
columns_to_check = ['Electricity_Reading','tavg']

for column in columns_to_check:
    skewness_value = t[column].skew()
    skewness_info[column] = skewness_value

# Display the skewness of each column
for column, skewness_value in skewness_info.items():
    print(f"Skewness of {column}: {skewness_value}")

import numpy as np

# Applying square root transformation
t['Electricity_Reading'] = np.sqrt(t['Electricity_Reading'])

# Check the new skewness values
print("Skewness after Square Root Transformation:")
print("Electricity_Reading:", t['Electricity_Reading'].skew())

df=t

df=df.drop(columns=['Date'])

print(column_mapping)

x=df.drop(columns=['Electricity_Reading'])

y=df['Electricity_Reading']

x.head()

y.head()

# Assuming 't' is your DataFrame
x = t[['Industry_Type', 'Number_of_Floors', 'Number_of_Employees', 'Day_of_Week', 'Acquired_Area', 'Month', 'tavg']]
y = t['Electricity_Reading']

# Train-Test Split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# List of models to evaluate
models = {
    "LGBMRegressor": LGBMRegressor(learning_rate=0.1, n_estimators=150),
    "RandomForestRegressor": RandomForestRegressor(n_estimators=150),
    "LinearRegression": LinearRegression(),
    "SVR": SVR()
}

# Store results
results = {
    'Model': [],
    'MSE': [],
    'R-squared': []
}

# Train, predict and evaluate each model
for model_name, model in models.items():
    # Train the model
    model.fit(x_train, y_train)


    # Predict
    y_pred = model.predict(x_test)

    # Evaluate
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # Store results
    results['Model'].append(model_name)
    results['MSE'].append(mse)
    results['R-squared'].append(r2)

    print(f"{model_name}:")
    print(f"Mean Squared Error: {mse}")
    print(f"R-squared: {r2}\n")

# Convert results to DataFrame for visualization
results_df = pd.DataFrame(results)

# Visualization
plt.figure(figsize=(12, 6))

# Custom color palette
mse_palette = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC66']  # Red, Blue, Green, Orange
r2_palette = ['#FF6666', '#6699FF', '#66FF66', '#FF9966']   # Darker shades for R-squared

# Plot MSE values
plt.subplot(1, 2, 1)
sns.barplot(x='Model', y='MSE', data=results_df, palette=mse_palette)
plt.title('Mean Squared Error (MSE) Comparison')
plt.xticks(rotation=45)

# Plot R-squared values
plt.subplot(1, 2, 2)
sns.barplot(x='Model', y='R-squared', data=results_df, palette=r2_palette)
plt.title('R-squared Comparison')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

# Sample tenant mapping
tenant_mapping = {
    0: 'ANZ', 1: 'Alcatel Lucent Technologies', 2: 'Andritz', 3: 'Cerner Health Care Solutions',
    4: 'Cognizant', 5: 'Colt Technologies', 6: 'Data Craft India', 7: 'Fidelity', 8: 'IBM',
    9: 'Jurimatrix', 10: 'Justdial', 11: 'L and T', 12: 'Mavenir', 13: 'Monsanto Holdings',
    14: 'Netscout System Software', 15: 'Nokia Siemens Network India', 16: 'Northern Operation Services',
    17: 'Novell', 18: 'Nvidia Graphics', 19: 'Philips', 20: 'Software Software Software',
    21: 'Stylus Commercial Services', 22: 'TCS', 23: 'TP Vision', 24: 'Target Operation', 25: 'Via.com'
}

# Assuming 't' is your DataFrame containing all tenant data
# Features and target variable
x = t[['Industry_Type', 'Number_of_Floors', 'Number_of_Employees', 'Day_of_Week', 'Acquired_Area', 'Month', 'tavg']]  # Features
y = t['Electricity_Reading']  # Target (Electricity usage)

# Step 1: Train-Test Split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Step 2: Train the model
model = LGBMRegressor(learning_rate=0.1, n_estimators=150)
model.fit(x_train, y_train)

# Step 3: Save the model
with open('electricity_model2.pkl', 'wb') as file:
    pickle.dump(model, file)

# Step 4: Load the model
with open('electricity_model2.pkl', 'rb') as file:
    loaded_model = pickle.load(file)

# Step 5: Evaluate the model
y_pred = loaded_model.predict(x_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

# Step 6: Predict electricity readings for the current month for each tenant

# Initialize an empty list to store forecasts for all tenants
current_month_forecasts = []

# Get the current date
current_date = datetime.today()

# Calculate the first day of the current month
first_day_current_month = current_date.replace(day=1)

# Calculate the last day of the current month
last_day_current_month = (first_day_current_month + timedelta(days=32)).replace(day=1) - timedelta(days=1)

# Generate dates for the current month
current_month_dates = pd.date_range(start=first_day_current_month, end=last_day_current_month, freq='D')

# Iterate over each unique tenant and generate data for each
for tenant_code in t['Tenant'].unique():
    # Get tenant-specific information
    tenant_info = t[t['Tenant'] == tenant_code].iloc[0]

    # Prepare data for each tenant for the current month
    current_month_data_tenant = pd.DataFrame({
        'Tenant': [tenant_code] * len(current_month_dates),  # Include the tenant code
        'Date': current_month_dates,  # Dates for the current month
    })

    # Generate predictions using the trained model (assuming 'model' is already trained)
    # Extract the features needed for prediction
    features_for_prediction = pd.DataFrame({
        'Industry_Type': [tenant_info['Industry_Type']] * len(current_month_dates),
        'Number_of_Floors': [tenant_info['Number_of_Floors']] * len(current_month_dates),
        'Number_of_Employees': [tenant_info['Number_of_Employees']] * len(current_month_dates),
        'Day_of_Week': current_month_dates.dayofweek + 1,  # Day of the week (1 = Monday, 7 = Sunday)
        'Acquired_Area': [tenant_info['Acquired_Area']] * len(current_month_dates),
        'Month': [current_month_dates.month[0]] * len(current_month_dates),  # Current month
        'tavg': [25] * len(current_month_dates),  # Average temperature for simplicity
    })

    # Predict electricity readings for each day of the month
    predicted_electricity_readings = model.predict(features_for_prediction)

    # Add the predicted electricity readings to the tenant's DataFrame
    current_month_data_tenant['Predicted_Electricity_Reading (kWh)'] = predicted_electricity_readings

    # Append to the forecast list
    current_month_forecasts.append(current_month_data_tenant)

# Combine all tenant data into one DataFrame
current_month_forecast_data = pd.concat(current_month_forecasts, ignore_index=True)

# Function to map tenant IDs to names
def map_tenant_names(tenant_id):
    return tenant_mapping.get(tenant_id, "Unknown Tenant")  # Default if not found

# Map tenant names to the DataFrame
current_month_forecast_data['Tenant_Name'] = current_month_forecast_data['Tenant'].apply(map_tenant_names)

# Optionally, drop the 'Tenant' column if you only want the names
current_month_forecast_data = current_month_forecast_data[['Tenant_Name', 'Date', 'Predicted_Electricity_Reading (kWh)']]

# Display the first few rows of the generated data for the current month
print(current_month_forecast_data.head())

# Step 5: Evaluate the model
y_train_pred = model.predict(x_train)
y_test_pred = model.predict(x_test)

# Calculate metrics for training and testing
train_mse = mean_squared_error(y_train, y_train_pred)
train_r2 = r2_score(y_train, y_train_pred)

test_mse = mean_squared_error(y_test, y_test_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"Training Mean Squared Error: {train_mse}")
print(f"Training R-squared: {train_r2}")
print(f"Testing Mean Squared Error: {test_mse}")
print(f"Testing R-squared: {test_r2}")

"""**Not Underfitting**: Since both the training and testing R-squared values are very high, the model is **not underfitting**. Underfitting would show both R-squared values significantly lower.

**Not Overfitting**: The model performs well on both training and testing datasets. Although the training MSE is lower than the testing MSE, the difference is not large, and both metrics are acceptable. In general, a small gap between training and testing performance indicates that the model generalizes well.
"""

current_month_forecast_data.to_csv('october_tenant_forecasts_final.csv', index=False)

# Load the model (assuming it's already trained and saved)
with open('electricity_model2.pkl', 'rb') as file:
    loaded_model = pickle.load(file)

# Assume 't' is your DataFrame containing all tenant data with recorded electricity readings
x = t[['Industry_Type', 'Number_of_Floors', 'Number_of_Employees', 'Day_of_Week', 'Acquired_Area', 'Month', 'tavg']]
y = t['Electricity_Reading']

# Split the data (if necessary)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Make predictions on the entire dataset (both train and test for overall comparison)
y_pred_overall = loaded_model.predict(x)

# Create a DataFrame to compare actual and predicted values
comparison_df = pd.DataFrame({
    'Date': t['Date'],  # Assuming you have a 'Date' column
    'Tenant': t['Tenant'].map(tenant_mapping),  # Map tenant codes to names
    'Actual_Electricity_Reading (kWh)': y,
    'Predicted_Electricity_Reading (kWh)': y_pred_overall
})

# Step 7: Plot the overall real vs. predicted electricity readings for all tenants
plt.figure(figsize=(12, 8))

# Group data by Date to visualize overall electricity usage across tenants
comparison_grouped = comparison_df.groupby('Date').sum().reset_index()

# Plot actual and predicted values across the dataset timeline
plt.plot(comparison_grouped['Date'], comparison_grouped['Actual_Electricity_Reading (kWh)'], label='Actual', linestyle='--', color='blue')
plt.plot(comparison_grouped['Date'], comparison_grouped['Predicted_Electricity_Reading (kWh)'], label='Predicted', color='red')

# Customize the plot
plt.title('Overall Real vs Predicted Electricity Readings')
plt.xlabel('Date')
plt.ylabel('Electricity Reading (kWh)')
plt.xticks(rotation=45)
plt.legend(loc='upper right')

# Display the plot
plt.tight_layout()
plt.show()

# Assuming the same 'comparison_df' from earlier
# Group data by Date to visualize overall electricity usage across tenants
comparison_grouped = comparison_df.groupby('Date').sum().reset_index()

# Plot actual and predicted values across the dataset timeline
plt.figure(figsize=(12, 8))

# Plot the actual values
plt.plot(comparison_grouped['Date'], comparison_grouped['Actual_Electricity_Reading (kWh)'],
         label='Actual', linestyle='--', color='blue')

# Plot the predicted values with transparency to avoid overlap issues
plt.plot(comparison_grouped['Date'], comparison_grouped['Predicted_Electricity_Reading (kWh)'],
         label='Predicted', color='red', alpha=0.6)  # Adding transparency with alpha

# Customize the plot
plt.title('Overall Real vs Predicted Electricity Readings')
plt.xlabel('Date')
plt.ylabel('Electricity Reading (kWh)')
plt.xticks(rotation=45)
plt.legend(loc='upper right')

# Display the plot
plt.tight_layout()
plt.show()

# Calculate residuals
comparison_grouped['Residuals'] = comparison_grouped['Actual_Electricity_Reading (kWh)'] - comparison_grouped['Predicted_Electricity_Reading (kWh)']

# Plot residuals
plt.figure(figsize=(12, 6))
plt.scatter(comparison_grouped['Date'], comparison_grouped['Residuals'], color='purple', alpha=0.5)
plt.axhline(0, linestyle='--', color='black')
plt.title('Residuals Plot: Actual vs Predicted Electricity Readings')
plt.xlabel('Date')
plt.ylabel('Residuals (Actual - Predicted)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

""" This plot shows the difference between the actual electricity readings and the predicted values. The ideal scenario is to have the residuals randomly scattered around zero, indicating that the model is not systematically over or underpredicting. **The graph shows that the residuals are mostly randomly scattered, but there are some patterns that suggest the model may not be perfect.**"""

plt.figure(figsize=(8, 6))
sns.histplot(comparison_grouped['Residuals'], bins=30, kde=True, color='green')
plt.title('Distribution of Residuals (Errors)')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.tight_layout()
plt.show()

"""Here are a few interpretations based on the plot:

**Symmetry**: The residuals are symmetrically distributed around zero, suggesting that the errors are fairly distributed across the predictions. This is a good sign for model accuracy.

**Bell-shaped curve**: The normality (bell-shaped curve) of the residuals means the model errors are mostly small and follow a normal distribution, which is what you generally aim for in regression models.

**Outliers**: There are some residuals on both tails of the distribution, but they appear to be relatively few, meaning the majority of your predictions are close to the actual values.
"""

plt.figure(figsize=(8, 6))
plt.scatter(comparison_grouped['Actual_Electricity_Reading (kWh)'], comparison_grouped['Predicted_Electricity_Reading (kWh)'],
            color='orange', alpha=0.5)
plt.plot([comparison_grouped['Actual_Electricity_Reading (kWh)'].min(), comparison_grouped['Actual_Electricity_Reading (kWh)'].max()],
         [comparison_grouped['Actual_Electricity_Reading (kWh)'].min(), comparison_grouped['Actual_Electricity_Reading (kWh)'].max()],
         color='red', linestyle='--')  # Line showing perfect prediction
plt.title('Scatter Plot: Actual vs Predicted Electricity Readings')
plt.xlabel('Actual Electricity Reading (kWh)')
plt.ylabel('Predicted Electricity Reading (kWh)')
plt.tight_layout()
plt.show()

"""This plot compares the actual electricity readings against the predicted readings. A perfect model would have all points lie on the diagonal line. **The graph shows that the model is generally accurate, with points clustering around the diagonal line.**"""

comparison_grouped['Actual_Rolling'] = comparison_grouped['Actual_Electricity_Reading (kWh)'].rolling(window=30).mean()
comparison_grouped['Predicted_Rolling'] = comparison_grouped['Predicted_Electricity_Reading (kWh)'].rolling(window=30).mean()

plt.figure(figsize=(12, 6))
plt.plot(comparison_grouped['Date'], comparison_grouped['Actual_Rolling'], label='Actual Rolling Mean', color='blue')
plt.plot(comparison_grouped['Date'], comparison_grouped['Predicted_Rolling'], label='Predicted Rolling Mean', color='red')
plt.title('Rolling Mean Comparison: Actual vs Predicted Electricity Readings')
plt.xlabel('Date')
plt.ylabel('Electricity Reading (kWh)')
plt.xticks(rotation=45)
plt.legend(loc='upper right')
plt.tight_layout()
plt.show()

"""This plot shows the actual and predicted electricity readings, smoothed using a rolling mean. This helps to visualize the overall trend and how the model is capturing the seasonal patterns. **The graph shows that the model is generally capturing the overall trend and seasonal patterns, although there are some discrepancies.**"""

comparison_grouped['Cumulative_Error'] = comparison_grouped['Residuals'].cumsum()

plt.figure(figsize=(10, 6))
plt.plot(comparison_grouped['Date'], comparison_grouped['Cumulative_Error'], color='purple')
plt.title('Cumulative Error Over Time')
plt.xlabel('Date')
plt.ylabel('Cumulative Error (kWh)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

""" This plot shows the cumulative error of the model over time. It helps to visualize how the model is accumulating errors. **The graph shows that the cumulative error is fluctuating, but it does not seem to be diverging drastically, which is a good sign.**"""

# Create a column for year and week
comparison_grouped['Year'] = comparison_grouped['Date'].dt.year
comparison_grouped['Week'] = comparison_grouped['Date'].dt.isocalendar().week

# Pivot the data to create a heatmap of errors by week and year
error_pivot = comparison_grouped.pivot_table(values='Residuals', index='Week', columns='Year', aggfunc='mean')

plt.figure(figsize=(10, 8))
sns.heatmap(error_pivot, cmap='coolwarm', center=0)
plt.title('Heatmap of Prediction Errors Over Time')
plt.xlabel('Year')
plt.ylabel('Week of Year')
plt.tight_layout()
plt.show()